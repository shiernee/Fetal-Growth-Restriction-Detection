{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-486d8d36d5a6>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-486d8d36d5a6>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    pandas version 0.25.2\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# pandas version 0.25.2\n",
    "# numpy version 1.16.5\n",
    "# sklearn version 0.21.3\n",
    "# keras version 2.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas, numpy, sklearn, csv, itertools, os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV, train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report,confusion_matrix, auc, roc_curve, average_precision_score\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping, CSVLogger\n",
    "from keras.utils.np_utils import to_categorical \n",
    "from keras.models import model_from_json, Sequential\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile(filename1,filename2, filename3):\n",
    "    \n",
    "    names = ['Refno', 'BW (centile)', 'Status',  'Age', 'Ethnics', 'BPD', 'HC', 'Cerebellum_tr', 'CM', 'EFW', 'Nuchal_fold', \n",
    "             'AC', 'Femur', 'Va', 'Vp', 'Hem', 'RI (Right)', 'PI (Right)', 'GA_wk', 'GA_day', 'GA_scan_wk']\n",
    "        \n",
    "    dataframe = pandas.read_csv(filename1,skiprows=1, delimiter='\\,' , names=names, index_col=False )\n",
    "    dataframe = dataframe.dropna() \n",
    "    array = dataframe.values\n",
    "    array[:,2] = numpy.round(array[:,2])\n",
    "    X_train_resampled =array\n",
    "    Y_train_resampled = numpy.abs(array[:,2])\n",
    "\n",
    "    # load testing file\n",
    "    dataframe = pandas.read_csv(filename2,skiprows=1, delimiter='\\,' , names=names, index_col=False )\n",
    "    dataframe = dataframe.dropna() \n",
    "    array = dataframe.values\n",
    "    array[:,2] = numpy.round(array[:,2])\n",
    "    array[:,2] = numpy.abs(array[:,2])\n",
    "    X_test = array\n",
    "    Y_test = array[:, 2]\n",
    "\n",
    "    # load training before oversampled to get the scaler metrics\n",
    "    dataframe = pandas.read_csv(filename3,skiprows=1, delimiter='\\,' , names=names, index_col=False )\n",
    "    dataframe = dataframe.dropna() \n",
    "    array = dataframe.values\n",
    "    xdata = array\n",
    "    scaler = StandardScaler().fit(xdata)\n",
    "\n",
    "    X_train_resampled_scaled = scaler.transform(X_train_resampled)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train_resampled_scaled, X_test_scaled, Y_train_resampled, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CV_RF(X,X_test,Y_train_resampled, Y_test, filetosave):\n",
    "    param_grid = {'n_estimators': [10,20,30], 'max_depth': [4,5,6,7]}\n",
    "\n",
    "    clf0 = RandomForestClassifier(oob_score=False, random_state=None) \n",
    "    grid_clf0 = GridSearchCV(clf0, param_grid, cv=5,return_train_score=True)\n",
    "    grid_clf0.fit(X, Y_train_resampled)\n",
    "    \n",
    "    towritedata = pandas.DataFrame(grid_clf0.cv_results_)\n",
    "    towritedata.to_csv(filetosave + '\\RF_CVResults1.csv')\n",
    "\n",
    "    clf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=1,\n",
    "            oob_score=False, verbose=0, warm_start=False)\n",
    "\n",
    "    best_param = list(grid_clf0.best_params_.items())\n",
    "    \n",
    "    clf.set_params(**grid_clf0.best_params_)\n",
    "    \n",
    "    clf.fit(X, Y_train_resampled)  \n",
    "\n",
    "    importance = clf.feature_importances_\n",
    "        \n",
    "    n_classes = 2\n",
    "    y_predict_prob =  clf.predict_proba(X_test)\n",
    "\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(Y_test, y_predict_prob[:,1])\n",
    "\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    joblib.dump(clf, filetosave + '\\RF1.sav')\n",
    "    \n",
    "    return best_param[0][1], best_param[1][1], importance, roc_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CV_SVM(X,X_test,Y_train_resampled, Y_test, filetosave):\n",
    "    \n",
    "    SVM_model = svm.SVC()\n",
    "    param_grid_linear = [{'kernel': ['linear'], 'C': numpy.arange(0.1, 10.0, 0.1)}]\n",
    "\n",
    "    grid = GridSearchCV(estimator= SVM_model, param_grid = param_grid_linear, scoring=None, n_jobs=1, \n",
    "                        iid=True, refit=True, cv=5, verbose=1, pre_dispatch='2*n_jobs', error_score='raise', \n",
    "                        return_train_score=True)      #cv=None=3-fold cv\n",
    "\n",
    "    grid.fit(X, Y_train_resampled)\n",
    "    \n",
    "    best_params = list(grid.best_params_.items())\n",
    "    \n",
    "    towritedata = pandas.DataFrame(grid.cv_results_)\n",
    "    towritedata.to_csv(filetosave + '\\SVM_CVResults.csv')\n",
    "    \n",
    "    SVM_model1 = svm.SVC(C=1.0, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True, \n",
    "                    probability=True, tol=0.001, cache_size=200, class_weight=None, \n",
    "                    verbose=False, max_iter=-1, decision_function_shape='ovr', random_state=None)\n",
    "\n",
    "    SVM_model1.set_params(**grid.best_params_)\n",
    "    X = X_train_resampled_scaled[:, sequence]\n",
    "    SVM_model1.fit(X, Y_train_resampled)\n",
    "    \n",
    "    importance = SVM_model1.coef_\n",
    "\n",
    "    joblib.dump(SVM_model1, filetosave + '\\SVM.sav')\n",
    "    \n",
    "    n_classes = 2\n",
    "    y_predict_prob =  SVM_model1.predict_proba(X_test)\n",
    "\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(Y_test, y_predict_prob[:,1])\n",
    "\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    return best_params[0], importance, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CV_MLP(X,X_test,Y_train_resampled, Y_test, filetosave):\n",
    "    \n",
    "    Y_test_MLP = to_categorical(Y_test, num_classes=2)\n",
    "    Y_train_resampled_MLP = to_categorical(Y_train_resampled, num_classes=2)\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=None)\n",
    "    cvscores0 = []\n",
    "    cvscores1 = []\n",
    "    cvscores2 = []\n",
    "    meancv = []\n",
    "    \n",
    "    categorical_accuracy = 0\n",
    "    k = 0\n",
    "    epoch = 1\n",
    "\n",
    "    for train, test in kfold.split(X, Y_train_resampled):\n",
    "        \n",
    "        X_train = X[train]\n",
    "        X_valid = X[test]\n",
    "        Y_train = Y_train_resampled_MLP[train]\n",
    "        Y_valid = Y_train_resampled_MLP[test]    \n",
    "     \n",
    "        model = MLP_network(X_train, 0)\n",
    "        csv_logger = CSVLogger(filetosave +  '\\lr=0.001_MLP0' + '_' + str(k) + 'fold' + '_metric.csv', append=True, \n",
    "                               separator=',')\n",
    "        model.fit(X_train, Y_train, epochs=epoch, validation_data = [X_valid, Y_valid], shuffle = True, batch_size=64, \n",
    "                  verbose=0, callbacks=[csv_logger])\n",
    "        cvscores0.append(evaluatemodel(model, X_valid, Y_valid))\n",
    "\n",
    "        model = MLP_network(X_train,1)\n",
    "        csv_logger = CSVLogger(filetosave +  '\\lr=0.001_MLP1' + '_' + str(k) + 'fold' + '_metric.csv', append=True,\n",
    "                               separator=',')\n",
    "        model.fit(X_train, Y_train, epochs=epoch, validation_data = [X_valid, Y_valid], shuffle = True, batch_size=64,\n",
    "                  verbose=0, callbacks=[csv_logger])\n",
    "        cvscores1.append(evaluatemodel(model, X_valid, Y_valid))\n",
    "        \n",
    "        model = MLP_network(X_train,2)\n",
    "        csv_logger = CSVLogger(filetosave +  '\\lr=0.001_MLP2' + '_' + str(k) + 'fold' + '_metric.csv', append=True,\n",
    "                               separator=',')\n",
    "        model.fit(X_train, Y_train, epochs=epoch, validation_data = [X_valid, Y_valid], shuffle = True, batch_size=64,\n",
    "                  verbose=0, callbacks=[csv_logger])\n",
    "        cvscores2.append(evaluatemodel(model, X_valid, Y_valid))\n",
    "        \n",
    "        k += 1\n",
    "        \n",
    "    meancv.append(numpy.mean(cvscores0))\n",
    "    meancv.append(numpy.mean(cvscores1))\n",
    "    meancv.append(numpy.mean(cvscores2))\n",
    "    max_index = meancv.index(max(meancv))\n",
    "\n",
    "    k = 99\n",
    "    model = MLP_network(X, max_index)\n",
    "    csv_logger = CSVLogger(filetosave +  '\\lr=0.001_MLP' + str(max_index) + '_' + str(k) + 'fold' + '_metric.csv', append=True,\n",
    "                           separator=',')\n",
    "    model.fit(X, Y_train_resampled_MLP, epochs=epoch, validation_split = 0.1, shuffle = True, batch_size=64, verbose=0, \n",
    "              callbacks=[csv_logger])\n",
    "    saveMLPmodel(model, filetosave + str('\\MLP') + str(max_index) + 'lr=0.001')\n",
    "\n",
    "    roc_auc = auroc(model, X_test, Y_test_MLP)\n",
    "        \n",
    "    return roc_auc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auroc(model, X_test, Y_test_MLP):\n",
    "    \n",
    "    n_classes = 2\n",
    "    y_predict_prob =  model.predict_proba(X_test)\n",
    "\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(Y_test_MLP[:,1], y_predict_prob[:,1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveMLPmodel(model, filetosave):\n",
    "    \n",
    "    MLPmodel_json = model.to_json()\n",
    "    with open(filetosave + \"lr=0.001.json\", \"w\") as json_file:\n",
    "        json_file.write(MLPmodel_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(filetosave + \"lr=0.001_weight\")\n",
    "    print(\"Saved model to disk\") \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP_network(X_train, network):\n",
    "    \n",
    "    input_dim = int(X_train.size/X_train.shape[0])\n",
    "    \n",
    "    if network == 0:\n",
    "        model = Sequential()\n",
    "        model.add(Dense(input_dim + 2, input_dim=input_dim, activation='relu'))\n",
    "        model.add(Dense(2, activation='softmax'))\n",
    "        \n",
    "    elif network == 1:\n",
    "        model = Sequential()\n",
    "        model.add(Dense(input_dim, input_dim=input_dim, activation='relu'))\n",
    "        model.add(Dense(2, activation='softmax'))\n",
    "        \n",
    "    elif network == 2: \n",
    "        model = Sequential()\n",
    "        model.add(Dense(input_dim + 4, input_dim=input_dim, activation='relu'))\n",
    "        model.add(Dense(input_dim + 2, activation='relu'))\n",
    "        model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "        \n",
    "    return model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluatemodel(model, X_valid, Y_valid):\n",
    "    \n",
    "    cvscores = []\n",
    "    scores = model.evaluate(X_valid, Y_valid, verbose=0)\n",
    "    cvscores.append(scores[1] * 100)\n",
    "    \n",
    "    return numpy.mean(cvscores)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createfolder(filetosave):\n",
    "\n",
    "    path = \"C:\\\\Users\\\\sawsn\\\\Desktop\\\\Shiernee\\\\2. Machine Learning Model\\\\AI_SGA_model\\\\output\" #os.getcwd()  \n",
    "    print (\"The current working directory is %s\" % path) \n",
    "    path = str(filetosave)\n",
    "    \n",
    "    if not os.path.exists(filetosave):    \n",
    "        try:  \n",
    "            os.mkdir(path)\n",
    "            print (\"Successfully created the directory %s \" % path)       \n",
    "        except OSError:  \n",
    "            print (\"Creation of the directory %s failed\" % path)\n",
    "    else:  \n",
    "        print (\"%s already exist\" % path)            \n",
    "        \n",
    "def writetofile(filename, header, datatowrite):\n",
    "    with filename:\n",
    "        writer = csv.writer(filename)\n",
    "        writer.writerow(header)\n",
    "        for items in datatowrite:\n",
    "            writer.writerow(items)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation on RF and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\SGA_trainingdata.csv\n",
      "[3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 20]\n",
      "The current working directory is C:\\Users\\sawsn\\Desktop\\Shiernee\\2. Machine Learning Model\\AI_SGA_model\\output\n",
      "Successfully created the directory output\\output_SGA_feature[3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 20] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sawsn\\Anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel_launcher.py:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n",
      "C:\\Users\\sawsn\\Anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel_launcher.py:14: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n",
      "C:\\Users\\sawsn\\Anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel_launcher.py:23: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "C:\\Users\\sawsn\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 99 candidates, totalling 495 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 495 out of 495 | elapsed:   16.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 17, 20]\n",
      "The current working directory is C:\\Users\\sawsn\\Desktop\\Shiernee\\2. Machine Learning Model\\AI_SGA_model\\output\n",
      "Successfully created the directory output\\output_SGA_feature[16, 17, 20] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sawsn\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 99 candidates, totalling 495 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 495 out of 495 | elapsed:    3.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 16, 17, 20]\n",
      "The current working directory is C:\\Users\\sawsn\\Desktop\\Shiernee\\2. Machine Learning Model\\AI_SGA_model\\output\n",
      "Successfully created the directory output\\output_SGA_feature[9, 16, 17, 20] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sawsn\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 99 candidates, totalling 495 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 495 out of 495 | elapsed:    3.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 11, 16, 17, 20]\n",
      "The current working directory is C:\\Users\\sawsn\\Desktop\\Shiernee\\2. Machine Learning Model\\AI_SGA_model\\output\n",
      "Successfully created the directory output\\output_SGA_feature[9, 11, 16, 17, 20] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sawsn\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 99 candidates, totalling 495 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 495 out of 495 | elapsed:    4.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 6, 11, 12, 16, 17, 20]\n",
      "The current working directory is C:\\Users\\sawsn\\Desktop\\Shiernee\\2. Machine Learning Model\\AI_SGA_model\\output\n",
      "Successfully created the directory output\\output_SGA_feature[9, 6, 11, 12, 16, 17, 20] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sawsn\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 99 candidates, totalling 495 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 495 out of 495 | elapsed:    4.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 16, 17, 20]\n",
      "The current working directory is C:\\Users\\sawsn\\Desktop\\Shiernee\\2. Machine Learning Model\\AI_SGA_model\\output\n",
      "Successfully created the directory output\\output_SGA_feature[10, 16, 17, 20] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sawsn\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 99 candidates, totalling 495 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 495 out of 495 | elapsed:    3.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 9, 16, 17, 20]\n",
      "The current working directory is C:\\Users\\sawsn\\Desktop\\Shiernee\\2. Machine Learning Model\\AI_SGA_model\\output\n",
      "Successfully created the directory output\\output_SGA_feature[10, 9, 16, 17, 20] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sawsn\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 99 candidates, totalling 495 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 495 out of 495 | elapsed:    3.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 9, 11, 16, 17, 20]\n",
      "The current working directory is C:\\Users\\sawsn\\Desktop\\Shiernee\\2. Machine Learning Model\\AI_SGA_model\\output\n",
      "Successfully created the directory output\\output_SGA_feature[10, 9, 11, 16, 17, 20] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sawsn\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 99 candidates, totalling 495 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 495 out of 495 | elapsed:    4.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 9, 6, 11, 12, 16, 17, 20]\n",
      "The current working directory is C:\\Users\\sawsn\\Desktop\\Shiernee\\2. Machine Learning Model\\AI_SGA_model\\output\n",
      "Successfully created the directory output\\output_SGA_feature[10, 9, 6, 11, 12, 16, 17, 20] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sawsn\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 99 candidates, totalling 495 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 495 out of 495 | elapsed:    4.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 20]\n",
      "The current working directory is C:\\Users\\sawsn\\Desktop\\Shiernee\\2. Machine Learning Model\\AI_SGA_model\\output\n",
      "Successfully created the directory output\\output_SGA_feature[10, 20] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sawsn\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 99 candidates, totalling 495 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 495 out of 495 | elapsed:    2.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 20]\n",
      "The current working directory is C:\\Users\\sawsn\\Desktop\\Shiernee\\2. Machine Learning Model\\AI_SGA_model\\output\n",
      "Successfully created the directory output\\output_SGA_feature[3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 20] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sawsn\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 99 candidates, totalling 495 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 495 out of 495 | elapsed:   14.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 11, 17, 20]\n",
      "The current working directory is C:\\Users\\sawsn\\Desktop\\Shiernee\\2. Machine Learning Model\\AI_SGA_model\\output\n",
      "Successfully created the directory output\\output_SGA_feature[10, 11, 17, 20] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sawsn\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 99 candidates, totalling 495 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 495 out of 495 | elapsed:    4.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 10, 11, 17, 20]\n",
      "The current working directory is C:\\Users\\sawsn\\Desktop\\Shiernee\\2. Machine Learning Model\\AI_SGA_model\\output\n",
      "Successfully created the directory output\\output_SGA_feature[9, 10, 11, 17, 20] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sawsn\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 99 candidates, totalling 495 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 495 out of 495 | elapsed:    4.4s finished\n",
      "C:\\Users\\sawsn\\Anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel_launcher.py:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n",
      "C:\\Users\\sawsn\\Anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel_launcher.py:14: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n",
      "C:\\Users\\sawsn\\Anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel_launcher.py:23: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n",
      "data\\severeSGA_trainingdata.csv\n",
      "[3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 20]\n",
      "The current working directory is C:\\Users\\sawsn\\Desktop\\Shiernee\\2. Machine Learning Model\\AI_SGA_model\\output\n",
      "Successfully created the directory output\\output_severeSGA_feature[3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 20] \n",
      "Fitting 5 folds for each of 99 candidates, totalling 495 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 495 out of 495 | elapsed:    7.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 17, 20]\n",
      "The current working directory is C:\\Users\\sawsn\\Desktop\\Shiernee\\2. Machine Learning Model\\AI_SGA_model\\output\n",
      "Successfully created the directory output\\output_severeSGA_feature[16, 17, 20] \n",
      "Fitting 5 folds for each of 99 candidates, totalling 495 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 495 out of 495 | elapsed:    1.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 16, 17, 20]\n",
      "The current working directory is C:\\Users\\sawsn\\Desktop\\Shiernee\\2. Machine Learning Model\\AI_SGA_model\\output\n",
      "Successfully created the directory output\\output_severeSGA_feature[9, 16, 17, 20] \n",
      "Fitting 5 folds for each of 99 candidates, totalling 495 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 495 out of 495 | elapsed:    2.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 11, 16, 17, 20]\n",
      "The current working directory is C:\\Users\\sawsn\\Desktop\\Shiernee\\2. Machine Learning Model\\AI_SGA_model\\output\n",
      "Successfully created the directory output\\output_severeSGA_feature[9, 11, 16, 17, 20] \n",
      "Fitting 5 folds for each of 99 candidates, totalling 495 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 495 out of 495 | elapsed:    3.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 6, 11, 12, 16, 17, 20]\n",
      "The current working directory is C:\\Users\\sawsn\\Desktop\\Shiernee\\2. Machine Learning Model\\AI_SGA_model\\output\n",
      "Successfully created the directory output\\output_severeSGA_feature[9, 6, 11, 12, 16, 17, 20] \n",
      "Fitting 5 folds for each of 99 candidates, totalling 495 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 495 out of 495 | elapsed:    2.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 16, 17, 20]\n",
      "The current working directory is C:\\Users\\sawsn\\Desktop\\Shiernee\\2. Machine Learning Model\\AI_SGA_model\\output\n",
      "Successfully created the directory output\\output_severeSGA_feature[10, 16, 17, 20] \n",
      "Fitting 5 folds for each of 99 candidates, totalling 495 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 495 out of 495 | elapsed:    2.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 9, 16, 17, 20]\n",
      "The current working directory is C:\\Users\\sawsn\\Desktop\\Shiernee\\2. Machine Learning Model\\AI_SGA_model\\output\n",
      "Successfully created the directory output\\output_severeSGA_feature[10, 9, 16, 17, 20] \n",
      "Fitting 5 folds for each of 99 candidates, totalling 495 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 495 out of 495 | elapsed:    3.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 9, 11, 16, 17, 20]\n",
      "The current working directory is C:\\Users\\sawsn\\Desktop\\Shiernee\\2. Machine Learning Model\\AI_SGA_model\\output\n",
      "Successfully created the directory output\\output_severeSGA_feature[10, 9, 11, 16, 17, 20] \n",
      "Fitting 5 folds for each of 99 candidates, totalling 495 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 495 out of 495 | elapsed:    7.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 9, 6, 11, 12, 16, 17, 20]\n",
      "The current working directory is C:\\Users\\sawsn\\Desktop\\Shiernee\\2. Machine Learning Model\\AI_SGA_model\\output\n",
      "Successfully created the directory output\\output_severeSGA_feature[10, 9, 6, 11, 12, 16, 17, 20] \n",
      "Fitting 5 folds for each of 99 candidates, totalling 495 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 495 out of 495 | elapsed:    4.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 20]\n",
      "The current working directory is C:\\Users\\sawsn\\Desktop\\Shiernee\\2. Machine Learning Model\\AI_SGA_model\\output\n",
      "Successfully created the directory output\\output_severeSGA_feature[10, 20] \n",
      "Fitting 5 folds for each of 99 candidates, totalling 495 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 495 out of 495 | elapsed:    2.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 20]\n",
      "The current working directory is C:\\Users\\sawsn\\Desktop\\Shiernee\\2. Machine Learning Model\\AI_SGA_model\\output\n",
      "Successfully created the directory output\\output_severeSGA_feature[3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 20] \n",
      "Fitting 5 folds for each of 99 candidates, totalling 495 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 495 out of 495 | elapsed:    8.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 11, 17, 20]\n",
      "The current working directory is C:\\Users\\sawsn\\Desktop\\Shiernee\\2. Machine Learning Model\\AI_SGA_model\\output\n",
      "Successfully created the directory output\\output_severeSGA_feature[10, 11, 17, 20] \n",
      "Fitting 5 folds for each of 99 candidates, totalling 495 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 495 out of 495 | elapsed:    3.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 10, 11, 17, 20]\n",
      "The current working directory is C:\\Users\\sawsn\\Desktop\\Shiernee\\2. Machine Learning Model\\AI_SGA_model\\output\n",
      "Successfully created the directory output\\output_severeSGA_feature[9, 10, 11, 17, 20] \n",
      "Fitting 5 folds for each of 99 candidates, totalling 495 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 495 out of 495 | elapsed:    3.0s finished\n"
     ]
    }
   ],
   "source": [
    "list_of_features_combinations = [[3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,20],\n",
    "                                 [16,17,20],\n",
    "                                 [9,16,17,20],\n",
    "                                 [9,11,16,17,20],\n",
    "                                 [9,6,11,12,16,17,20],\n",
    "                                 [10,16,17,20],\n",
    "                                 [10,9,16,17,20],\n",
    "                                 [10,9,11,16,17,20],\n",
    "                                 [10,9,6,11,12,16,17,20],                               \n",
    "                                 [10,20],\n",
    "                                 [3,4,5,6,7,8,9,11,12,13,14,15,16,17,20],\n",
    "                                 [10,11,17,20],\n",
    "                                 [9,10, 11, 17, 20]]\n",
    "\n",
    "\n",
    "roc_auc = numpy.zeros((3,len(list_of_features_combinations)))\n",
    "feature_importance = numpy.zeros((len(list_of_features_combinations),21))\n",
    "feature_importance_svm = numpy.zeros((len(list_of_features_combinations),21))\n",
    "best_param_RF = []\n",
    "\n",
    "name_ = ['SGA', 'severeSGA']\n",
    "for i in name_:\n",
    "    loc = \"data\\\\\"\n",
    "    filename1 = loc + i + '_trainingdata.csv'\n",
    "    filename2 = loc + i + '_testingdata.csv'\n",
    "    filename3 = loc + '29Dec2018_trainingdata_beforeoversampled_' + i + '.csv'\n",
    "    print(filename1)\n",
    "\n",
    "    [X_train_resampled_scaled, X_test_scaled, Y_train_resampled, Y_test] = readfile(filename1, filename2, filename3)\n",
    "    \n",
    "    j=0\n",
    "    \n",
    "    for features in list_of_features_combinations:\n",
    "              \n",
    "        sequence = features\n",
    "        print(sequence)\n",
    "        X = X_train_resampled_scaled[:, sequence]\n",
    "        X_test = X_test_scaled[:, sequence]\n",
    "        \n",
    "        filetosave = 'output\\\\' + 'output_' + i + '_feature' + str(features)   \n",
    "        createfolder(filetosave)\n",
    "        \n",
    "        [depth, n_estimator, importance, roc_auc[0,j]] = CV_RF(X,X_test,Y_train_resampled, Y_test, filetosave)\n",
    "        feature_importance[j,sequence] = importance\n",
    "        [best_param_SVM, importance_svm, roc_auc[1,j]] = CV_SVM(X,X_test,Y_train_resampled, Y_test, filetosave)\n",
    "        feature_importance_svm[j,sequence] = importance_svm\n",
    "        best_param_RF.append([depth, n_estimator, best_param_SVM])\n",
    "        \n",
    "        j += 1 \n",
    "        \n",
    "    myFile = open(i + '_RFimportance1.csv', 'w' ,newline='')             \n",
    "    header = ['Refno', 'BW (centile)', 'Status',  'Age', 'Ethnics', 'BPD', 'HC', 'Cerebellum_tr', 'CM', 'EFW', 'Nuchal_fold', \n",
    "             'AC', 'Femur', 'Va', 'Vp', 'Hem', 'RI (Right)', 'PI (Right)', 'GA_wk', 'GA_day', 'GA_scan_wk']\n",
    "    writetofile(myFile, header, feature_importance)\n",
    "    \n",
    "    \n",
    "    myFile = open(i + '_SVMimportance1.csv', 'w' ,newline='')             \n",
    "    header = ['Refno', 'BW (centile)', 'Status',  'Age', 'Ethnics', 'BPD', 'HC', 'Cerebellum_tr', 'CM', 'EFW', 'Nuchal_fold', \n",
    "             'AC', 'Femur', 'Va', 'Vp', 'Hem', 'RI (Right)', 'PI (Right)', 'GA_wk', 'GA_day', 'GA_scan_wk']\n",
    "    writetofile(myFile, header, feature_importance_svm)\n",
    "\n",
    "    myFile = open(i + '_auroc1.csv', 'w' ,newline='')             \n",
    "    header = list_of_features_combinations\n",
    "    writetofile(myFile, header, roc_auc)\n",
    "    \n",
    "    myFile = open(i + '_RFparameter1.csv', 'w' ,newline='')             \n",
    "    header = ['depth', ' no_estimator', 'SVM_C']\n",
    "    writetofile(myFile, header, best_param_RF)\n",
    "    \n",
    "    print('DONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV on MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\SGA_trainingdata.csv\n",
      "[3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 20]\n",
      "The current working directory is C:\\Users\\sawsn\\Desktop\\Shiernee\\2. Machine Learning Model\\AI_SGA_model\\output\n",
      "output\\output_SGA_feature[3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 20] already exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sawsn\\Anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel_launcher.py:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n",
      "C:\\Users\\sawsn\\Anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel_launcher.py:14: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n",
      "C:\\Users\\sawsn\\Anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel_launcher.py:23: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n",
      "[16, 17, 20]\n",
      "The current working directory is C:\\Users\\sawsn\\Desktop\\Shiernee\\2. Machine Learning Model\\AI_SGA_model\\output\n",
      "output\\output_SGA_feature[16, 17, 20] already exist\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-ad6187fdd661>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mcreatefolder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiletosave\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mroc_auc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCV_MLP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_train_resampled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiletosave\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[0mj\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-46-f3bcc879470b>\u001b[0m in \u001b[0;36mCV_MLP\u001b[1;34m(X, X_test, Y_train_resampled, Y_test, filetosave)\u001b[0m\n\u001b[0;32m     32\u001b[0m                                separator=',')\n\u001b[0;32m     33\u001b[0m         model.fit(X_train, Y_train, epochs=epoch, validation_data = [X_valid, Y_valid], shuffle = True, batch_size=64,\n\u001b[1;32m---> 34\u001b[1;33m                   verbose=0, callbacks=[csv_logger])\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[0mcvscores1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluatemodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    210\u001b[0m                         val_outs = test_loop(model, val_f, val_ins,\n\u001b[0;32m    211\u001b[0m                                              \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m                                              verbose=0)\n\u001b[0m\u001b[0;32m    213\u001b[0m                         \u001b[0mval_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m                         \u001b[1;31m# Same labels assumed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m    390\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 392\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    393\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2669\u001b[0m                                 \u001b[0mfeed_symbols\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2670\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2671\u001b[1;33m                                 session)\n\u001b[0m\u001b[0;32m   2672\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_make_callable\u001b[1;34m(self, feed_arrays, feed_symbols, symbol_vals, session)\u001b[0m\n\u001b[0;32m   2621\u001b[0m             \u001b[0mcallable_opts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2622\u001b[0m         \u001b[1;31m# Create callable.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2623\u001b[1;33m         \u001b[0mcallable_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_callable_from_options\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallable_opts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2624\u001b[0m         \u001b[1;31m# Cache parameters corresponding to the generated callable, so that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2625\u001b[0m         \u001b[1;31m# we can detect future mismatches and refresh the callable.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_make_callable_from_options\u001b[1;34m(self, callable_options)\u001b[0m\n\u001b[0;32m   1487\u001b[0m     \"\"\"\n\u001b[0;32m   1488\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1489\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mBaseSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallable_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1491\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, session, callable_options)\u001b[0m\n\u001b[0;32m   1444\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1445\u001b[0m         self._handle = tf_session.TF_SessionMakeCallable(\n\u001b[1;32m-> 1446\u001b[1;33m             session._session, options_ptr)\n\u001b[0m\u001b[0;32m   1447\u001b[0m       \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1448\u001b[0m         \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_DeleteBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptions_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "list_of_features_combinations = [[3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,20],\n",
    "                                 [16,17,20],\n",
    "                                 [9,16,17,20],\n",
    "                                 [9,11,16,17,20],\n",
    "                                 [9,6,11,12,16,17,20],\n",
    "                                 [10,16,17,20],\n",
    "                                 [10,9,16,17,20],\n",
    "                                 [10,9,11,16,17,20],\n",
    "                                 [10,9,6,11,12,16,17,20],                               \n",
    "                                 [10,20],\n",
    "                                 [3,4,5,6,7,8,9,11,12,13,14,15,16,17,20],\n",
    "                                 [10,11,17,20],\n",
    "                                 [9,10, 11, 17, 20]]\n",
    "\n",
    "feature_importance = numpy.zeros((len(list_of_features_combinations),21))\n",
    "best_param_RF = []\n",
    "\n",
    "name_ = ['SGA', 'severeSGA']\n",
    "for i in name_:\n",
    "    loc = \"data\\\\\"\n",
    "    filename1 = loc + i + '_trainingdata.csv'\n",
    "    filename2 = loc + i + '_testingdata.csv'\n",
    "    filename3 = loc + '29Dec2018_trainingdata_beforeoversampled_' + i + '.csv'\n",
    "    print(filename1)\n",
    "    roc_auc = [] #numpy.zeros((len(list_of_features_combinations)))\n",
    "    \n",
    "    [X_train_resampled_scaled, X_test_scaled, Y_train_resampled, Y_test] = readfile(filename1, filename2, filename3)\n",
    "    \n",
    "    j=0\n",
    "    \n",
    "    for features in list_of_features_combinations:\n",
    "              \n",
    "        sequence = features\n",
    "        print(sequence)\n",
    "        X = X_train_resampled_scaled[:, sequence]\n",
    "        X_test = X_test_scaled[:, sequence]\n",
    "        \n",
    "        filetosave = 'output\\\\' + 'output_' + i + '_feature' + str(features)   \n",
    "        createfolder(filetosave)\n",
    "       \n",
    "        roc_auc.append(CV_MLP(X,X_test,Y_train_resampled, Y_test, filetosave))       \n",
    "        j += 1 \n",
    "        \n",
    "    myFile = open(i + '_auroc1.csv', 'a' ,newline='')             \n",
    "    header = list_of_features_combinations\n",
    "    with myFile:\n",
    "        writer = csv.writer(myFile) \n",
    "        writer.writerow(roc_auc)    \n",
    "        \n",
    "    print('DONE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RF and compute confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "import numpy, itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class ConfusionMatrix:\n",
    "    def __init__(self):\n",
    "        return\n",
    "\n",
    "    def sensitivity(self, FN, TP):\n",
    "        \"\"\"\n",
    "\n",
    "        :param FN: int\n",
    "        :param TP: int\n",
    "        :return: int\n",
    "        \"\"\"\n",
    "        return TP / (TP + FN)\n",
    "\n",
    "    def specificity(self, TN, FP):\n",
    "        \"\"\"\n",
    "\n",
    "        :param TN: int\n",
    "        :param FP: int\n",
    "        :return: int\n",
    "        \"\"\"\n",
    "        return TN / (TN + FP)\n",
    "\n",
    "    def accuracy(self, TN, FP, FN, TP):\n",
    "        \"\"\"\n",
    "\n",
    "        :param TN: int\n",
    "        :param FP: int\n",
    "        :param FN: int\n",
    "        :param TP: int\n",
    "        :return: int\n",
    "        \"\"\"\n",
    "        return (TN + TP) / (TN + FP + FN + TP)\n",
    "\n",
    "    def positive_LR(self, sensitivity, specificity):\n",
    "        \"\"\"\n",
    "\n",
    "        :param sensitivity: float\n",
    "        :param specificity: float\n",
    "        :return: float\n",
    "        \"\"\"\n",
    "        return sensitivity / (1 - specificity)\n",
    "\n",
    "    def negative_LR(self, sensitivity, specificity):\n",
    "        \"\"\"\n",
    "\n",
    "        :param sensitivity: float\n",
    "        :param specificity: float\n",
    "        :return: float\n",
    "        \"\"\"\n",
    "        return (1 - sensitivity) / specificity\n",
    "\n",
    "    def PPV(self, TP, FP):\n",
    "        \"\"\"\n",
    "\n",
    "        :param TP: int\n",
    "        :param FP: int\n",
    "        :return: int\n",
    "        \"\"\"\n",
    "        return TP / (TP + FP)\n",
    "\n",
    "    def NPV(self, TN, FN):\n",
    "        \"\"\"\n",
    "\n",
    "        :param TN: int\n",
    "        :param FN: int\n",
    "        :return: int\n",
    "        \"\"\"\n",
    "        return TN / (FN + TN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RF and compute statistic metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_ = ['SGA', 'severeSGA']\n",
    "\n",
    "for i in name_: \n",
    "    file_loc = \"data\\\\\"\n",
    "    filename1 = file_loc + i + '_trainingdata.csv'\n",
    "    filename2 = file_loc + i + '_testingdata.csv'\n",
    "    filename3 = file_loc + '29Dec2018_trainingdata_beforeoversampled_' + i + '.csv'\n",
    "    print(filename1)\n",
    "\n",
    "    fileio = FileIO()\n",
    "    [X_train_resampled_scaled, X_test_scaled, Y_train_resampled, Y_test] \\\n",
    "        = fileio.readfile(filename1, filename2, filename3)\n",
    "\n",
    "    stat = ConfusionMatrix()\n",
    "\n",
    "    writefile = numpy.zeros([numpy.shape(list_of_features_combinations)[0], 12])\n",
    "    TN, FP, FN, TP = numpy.array([]), numpy.array([]), numpy.array([]), numpy.array([])\n",
    "    sensitivity = numpy.array([])\n",
    "    specificity = numpy.array([])\n",
    "    accuracy = numpy.array([])\n",
    "    LRP = numpy.array([])\n",
    "    LRN = numpy.array([])\n",
    "    PPV = numpy.array([])\n",
    "    NPV = numpy.array([])\n",
    "\n",
    "    for j in range(len(list_of_features_combinations)):\n",
    "        sequence = list_of_features_combinations[j]\n",
    "        dir_ = \"output\\\\\" + 'output_' + i + '_feature' + str(sequence)\n",
    "        loaded_model_RF = joblib.load(dir_ + '\\RF1.sav')\n",
    "\n",
    "        X = X_test_scaled[:, sequence]\n",
    "        Y_predict_RF = loaded_model_RF.predict(X)\n",
    "        TN_tmp, FP_tmp, FN_tmp, TP_tmp = confusion_matrix(Y_test, Y_predict_RF).ravel()\n",
    "\n",
    "        sensitivity_tmp = stat.sensitivity(FN_tmp, TP_tmp)\n",
    "        specificity_tmp = stat.specificity(TN_tmp, FP_tmp)\n",
    "        accuracy_tmp = stat.accuracy(TN_tmp, FP_tmp, FN_tmp, TP_tmp)\n",
    "        LRP_tmp = stat.positive_LR(sensitivity_tmp, specificity_tmp)\n",
    "        LRN_tmp = stat.negative_LR(sensitivity_tmp, specificity_tmp)\n",
    "        PPV_tmp = stat.PPV(TP_tmp, FP_tmp)\n",
    "        NPV_tmp = stat.NPV(TN_tmp, FN_tmp)\n",
    "\n",
    "        writefile[j, 0] = TN_tmp\n",
    "        writefile[j, 1] = FP_tmp\n",
    "        writefile[j, 2] = FN_tmp\n",
    "        writefile[j, 3] = TP_tmp\n",
    "        writefile[j, 4] = sensitivity_tmp\n",
    "        writefile[j, 5] = specificity_tmp\n",
    "        writefile[j, 6] = accuracy_tmp\n",
    "        writefile[j, 7] = LRP_tmp\n",
    "        writefile[j, 8] = LRN_tmp\n",
    "        writefile[j, 9] = PPV_tmp\n",
    "        writefile[j, 10] = NPV_tmp\n",
    "\n",
    "    myFile = open(i + '_RF_cm1.csv', 'w', newline='')\n",
    "    header = ['TN', 'FP', 'FN', 'TP', 'sensitivity', 'specificity', 'accuracy', 'Positive LR',\n",
    "              'Negative LR', 'PPV', 'NPV']\n",
    "    with myFile:\n",
    "        writer = csv.writer(myFile)\n",
    "        writer.writerow(header)\n",
    "        for items in writefile:\n",
    "            writer.writerow(items)\n",
    "\n",
    "    print('RF CM DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SVM and compute statistic metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_ = ['SGA', 'severeSGA']\n",
    "\n",
    "for i in name_:  \n",
    "    file_loc =  \"data\\\\\"\n",
    "    filename1 = file_loc + i + '_trainingdata.csv'\n",
    "    filename2 = file_loc + i + '_testingdata.csv'\n",
    "    filename3 = file_loc + '29Dec2018_trainingdata_beforeoversampled_' + i + '.csv'\n",
    "    print(filename1)\n",
    "\n",
    "    fileio = FileIO()\n",
    "    [X_train_resampled_scaled, X_test_scaled, Y_train_resampled, Y_test] \\\n",
    "        = fileio.readfile(filename1, filename2, filename3)\n",
    "\n",
    "    stat = ConfusionMatrix()\n",
    "\n",
    "    writefile = numpy.zeros([numpy.shape(list_of_features_combinations)[0], 12])\n",
    "    TN, FP, FN, TP = numpy.array([]), numpy.array([]), numpy.array([]), numpy.array([])\n",
    "    sensitivity = numpy.array([])\n",
    "    specificity = numpy.array([])\n",
    "    accuracy = numpy.array([])\n",
    "    LRP = numpy.array([])\n",
    "    LRN = numpy.array([])\n",
    "    PPV = numpy.array([])\n",
    "    NPV = numpy.array([])\n",
    "\n",
    "    for j in range(len(list_of_features_combinations)):\n",
    "        sequence = list_of_features_combinations[j]\n",
    "        dir_ = file_loc = \"output\\\\\" + 'output_' + i + '_feature' + str(sequence)\n",
    "\n",
    "        loaded_model_SVM = joblib.load(dir_ + '\\SVM.sav')\n",
    "\n",
    "        X = X_test_scaled[:, sequence]\n",
    "        Y_predict_SVM = loaded_model_SVM.predict(X)\n",
    "        TN_tmp, FP_tmp, FP_tmp, TP_tmp = confusion_matrix(Y_test, Y_predict_SVM).ravel()\n",
    "\n",
    "        sensitivity_tmp = stat.sensitivity(FP_tmp, TP_tmp)\n",
    "        specificity_tmp = stat.specificity(TN_tmp, FP_tmp)\n",
    "        accuracy_tmp = stat.accuracy(TN_tmp, FP_tmp, FN_tmp, TP_tmp)\n",
    "        LRP_tmp = stat.positive_LR(sensitivity_tmp, specificity_tmp)\n",
    "        LRN_tmp = stat.negative_LR(sensitivity_tmp, specificity_tmp)\n",
    "        PPV_tmp = stat.PPV(TP_tmp, FP_tmp)\n",
    "        NPV_tmp = stat.NPV(TN_tmp, FP_tmp)\n",
    "\n",
    "        writefile[j, 0] = TN_tmp\n",
    "        writefile[j, 1] = FP_tmp\n",
    "        writefile[j, 2] = FN_tmp\n",
    "        writefile[j, 3] = TP_tmp\n",
    "        writefile[j, 4] = sensitivity_tmp\n",
    "        writefile[j, 5] = specificity_tmp\n",
    "        writefile[j, 6] = accuracy_tmp\n",
    "        writefile[j, 7] = LRP_tmp\n",
    "        writefile[j, 8] = LRN_tmp\n",
    "        writefile[j, 9] = PPV_tmp\n",
    "        writefile[j, 10] = NPV_tmp\n",
    "\n",
    "    myFile = open('T' + str(i + 5) + '_SVM_cm.csv', 'w', newline='')\n",
    "    header = ['TN', 'FP', 'FN', 'TP', 'sensitivity', 'specificity', 'accuracy', 'Positive LR',\n",
    "              'Negative LR', 'PPV', 'NPV']\n",
    "    with myFile:\n",
    "        writer = csv.writer(myFile)\n",
    "        writer.writerow(header)\n",
    "        for items in writefile:\n",
    "            writer.writerow(items)\n",
    "\n",
    "    print('SVM CM DONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MLP and compute statistic metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = [['MLP2', 'MLP1', 'MLP0', 'MLP2', 'MLP2', 'MLP0', 'MLP2', 'MLP0', 'MLP0', 'MLP2', 'MLP1', 'MLP0'],            \n",
    "         ['MLP2', 'MLP0', 'MLP1', 'MLP2', 'MLP1', 'MLP1', 'MLP0', 'MLP2', 'MLP2', 'MLP0', 'MLP2', 'MLP1']]\n",
    "\n",
    "name_ = ['SGA', 'severeSGA']\n",
    "\n",
    "for i in range(len(name_)): \n",
    "    file_loc =  \"data\\\\\"\n",
    "    filename1 = file_loc + name_[i] + '_trainingdata.csv'\n",
    "    filename2 = file_loc + name_[i] + '_testingdata.csv'\n",
    "    filename3 = file_loc + '29Dec2018_trainingdata_beforeoversampled_' + name_[i] + '.csv'\n",
    "    print(filename1)\n",
    "\n",
    "    fileio = FileIO()\n",
    "    [X_train_resampled_scaled, X_test_scaled, Y_train_resampled, Y_test] \\\n",
    "        = fileio.readfile(filename1, filename2, filename3)\n",
    "\n",
    "    stat = ConfusionMatrix()\n",
    "\n",
    "    writefile = numpy.zeros([numpy.shape(list_of_features_combinations)[0], 12])\n",
    "    TN, FP, FN, TP = numpy.array([]), numpy.array([]), numpy.array([]), numpy.array([])\n",
    "    sensitivity = numpy.array([])\n",
    "    specificity = numpy.array([])\n",
    "    accuracy = numpy.array([])\n",
    "    LRP = numpy.array([])\n",
    "    LRN = numpy.array([])\n",
    "    PPV = numpy.array([])\n",
    "    NPV = numpy.array([])\n",
    "\n",
    "    for j in range(len(list_of_features_combinations)):\n",
    "        sequence = list_of_features_combinations[j]\n",
    "        loc =  \"output\\\\\" + 'output_' + i + '_feature' + str(sequence)\n",
    "        filename = loc + dir_ + '\\\\' + dir_ + '\\\\' + str(model[i][j]) + 'lr=0.001lr=0.001.json'\n",
    "        weight_file = loc + dir_ + '\\\\' + dir_ + '\\\\' + str(model[i][j]) + 'lr=0.001lr=0.001_weight'\n",
    "        json_file = open(filename, 'r')\n",
    "\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "        # load weights into new model\n",
    "        loaded_model.load_weights(weight_file)\n",
    "        print(\"Loaded model from disk\")\n",
    "\n",
    "        X = X_test_scaled[:, sequence]\n",
    "        Y_predict = loaded_model.predict_classes(X, batch_size=10, verbose=1)\n",
    "        Y_test_MLP = to_categorical(Y_test, num_classes=2)\n",
    "        TN_tmp, FP_tmp, FN_tmp, TP_tmp = confusion_matrix(Y_test_MLP[:, 1], Y_predict).ravel()\n",
    "\n",
    "        sensitivity_tmp = stat.sensitivity(FN_tmp, TP_tmp)\n",
    "        specificity_tmp = stat.specificity(TN_tmp, FP_tmp)\n",
    "        accuracy_tmp = stat.accuracy(TN_tmp, FP_tmp, FN_tmp, TP_tmp)\n",
    "        LRP_tmp = stat.positive_LR(sensitivity_tmp, specificity_tmp)\n",
    "        LRN_tmp = stat.negative_LR(sensitivity_tmp, specificity_tmp)\n",
    "        PPV_tmp = stat.PPV(TP_tmp, FP_tmp)\n",
    "        NPV_tmp = stat.NPV(TN_tmp, FN_tmp)\n",
    "\n",
    "        writefile[j, 0] = TN_tmp\n",
    "        writefile[j, 1] = FP_tmp\n",
    "        writefile[j, 2] = FN_tmp\n",
    "        writefile[j, 3] = TP_tmp\n",
    "        writefile[j, 4] = sensitivity_tmp\n",
    "        writefile[j, 5] = specificity_tmp\n",
    "        writefile[j, 6] = accuracy_tmp\n",
    "        writefile[j, 7] = LRP_tmp\n",
    "        writefile[j, 8] = LRN_tmp\n",
    "        writefile[j, 9] = PPV_tmp\n",
    "        writefile[j, 10] = NPV_tmp\n",
    "\n",
    "    myFile = open(name_[i] '_MLP_cm.csv', 'w', newline='')\n",
    "    header = ['TN', 'FP', 'FN', 'TP', 'sensitivity', 'specificity', 'accuracy', 'Positive LR',\n",
    "              'Negative LR', 'PPV', 'NPV']\n",
    "    with myFile:\n",
    "        writer = csv.writer(myFile)\n",
    "        writer.writerow(header)\n",
    "        for items in writefile:\n",
    "            writer.writerow(items)\n",
    "\n",
    "    print('MLP CM DONE')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_features_combinations = [[3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,20]]\n",
    "\n",
    "feature_importance = numpy.zeros((len(list_of_features_combinations), 21))\n",
    "best_param_RF = []\n",
    "\n",
    "loc = \"data//\"\n",
    "filename1 = loc + 'SGA' + '_trainingdata.csv'\n",
    "filename2 = loc + 'SGA' + '_testingdata.csv'\n",
    "filename3 = loc + '29Dec2018_trainingdata_beforeoversampled_' + 'SGA' + '.csv'\n",
    "print(filename1)\n",
    "\n",
    "dataframe_training = pandas.read_csv(filename3,skiprows=1, delimiter='\\,' , names=names, index_col=False )\n",
    "dataframe_training = dataframe_training.dropna() \n",
    "\n",
    "dataframe_testing = pandas.read_csv(filename2,skiprows=1, delimiter='\\,' , names=names, index_col=False )\n",
    "dataframe_testing = dataframe_testing.dropna() \n",
    "\n",
    "names = ['Refno', 'BW (centile)', 'Status',  'Age', 'Ethnics', 'BPD', 'HC', 'Cerebellum_tr', 'CM', 'EFW', 'Nuchal_fold', \n",
    "             'AC', 'Femur', 'Va', 'Vp', 'Hem', 'RI (Right)', 'PI (Right)', 'GA_wk', 'GA_day', 'GA_scan_wk']\n",
    "\n",
    "dataframe_training.columns = names\n",
    "dataframe_testing.columns = names\n",
    "\n",
    "dataframe = pd.concat([dataframe_training, dataframe_testing], axis=0, ignore_index=True)\n",
    "\n",
    "print(dataframe_training.shape)\n",
    "print(dataframe_testing.shape)\n",
    "print(dataframe.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(dataframe.values)\n",
    "\n",
    "df = scaler.transform(dataframe.values)\n",
    "df = pd.DataFrame(df)\n",
    "df.columns = names\n",
    "df['Ethnics'] = dataframe['Ethnics']\n",
    "df['Status'] = dataframe['Status']\n",
    "df['BW (centile)'] = dataframe['BW (centile)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['NewStatus'] = df['BW (centile)'].apply(lambda x: 'AGA' if x > 10 else ('Severe SGA' if x < 3 else 'SGA'))\n",
    "df['Status'] = df['NewStatus']\n",
    "df.drop(['NewStatus'], axis=1, inplace=True)\n",
    "# df[['BW (centile)', 'NewStatus']]\n",
    "# plt.plot(df['NewStatus'], df['BW (centile)'], '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "ethnics = pd.get_dummies(df['Ethnics'], drop_first=True)\n",
    "ethnics.columns = ['Malay', 'Indian', 'Others']\n",
    "\n",
    "df.drop(['Ethnics'], axis=1, inplace=True)\n",
    "df = pd.concat([df, ethnics], axis=1)\n",
    "\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_to_drop=['Refno', 'BW (centile)','GA_wk', 'GA_day']\n",
    "df.drop(feature_to_drop, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_to_drop_for_vif = ['Status', 'EFW']\n",
    "vif_df = df.drop(feature_to_drop_for_vif,axis=1)\n",
    "print(vif_df.shape)\n",
    "print(vif_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif = pd.DataFrame()\n",
    "x_to_check = vif_df.values\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(x_to_check, i) for i in range(x_to_check.shape[1])]\n",
    "vif[\"features\"] = vif_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_to_drop_for_vif = ['EFW']\n",
    "X = df.drop(['Status'],axis=1)\n",
    "Y = df['Status']\n",
    "\n",
    "X.drop(feature_to_drop_for_vif, axis=1, inplace=True)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "mnlogit = sm.MNLogit(Y, X)\n",
    "\n",
    "# fit the model\n",
    "result = mnlogit.fit()\n",
    "print(result.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
